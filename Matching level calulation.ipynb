{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c6c417",
   "metadata": {},
   "source": [
    "### ⚠️ Note on Scope: U.S.-Only Implementation\n",
    "\n",
    "This notebook demonstrates the complete data processing and analysis workflow for **the United States (USA)**.\n",
    "\n",
    "While similar analyses have been conducted for **China** and **Europe**, only the U.S. version is included here for clarity and reproducibility.\n",
    "\n",
    "✅ **The methodology and code structure are consistent across all regions.**  \n",
    "The only differences lie in:\n",
    "- Input data files (e.g., CSVs, shapefiles)\n",
    "- Regional identifiers (e.g., county codes, administrative levels)\n",
    "- Data sources and file paths\n",
    "\n",
    "Therefore, this U.S. example serves as a **fully representative template** for understanding and replicating the analysis in other regions.\n",
    "\n",
    "All steps — including data merging, normalization, city-level aggregation, and matching level calculation — follow the same logic globally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e128f",
   "metadata": {},
   "source": [
    "# Maximum and minimum solar data loading in USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "\n",
    "# File paths\n",
    "tiff_file = 'USA_GISdata_LTAy_AvgDailyTotals_GlobalSolarAtlas-v2_GEOTIFF/western-hemisphere/GHI.tif'  # from External data\n",
    "city_boundaries_path = 'merged_city_boundariesUSCounties2.shp'  # from External data\n",
    "output_csv_file = 'US_cities_GHI_min_max1.csv'\n",
    "\n",
    "# Load city boundary data, specifying utf-8 encoding to prevent garbled text\n",
    "gdf_cities = gpd.read_file(city_boundaries_path, encoding='utf-8', engine='pyogrio')\n",
    "\n",
    "# Ensure that the CRS of the city boundary data matches that of the raster data\n",
    "# This step is necessary because zonal_stats requires vector and raster data to be in the same coordinate system\n",
    "gdf_cities = gdf_cities.to_crs(rasterio.open(tiff_file).crs)\n",
    "\n",
    "# Calculate the maximum and minimum values of GHI within each city area\n",
    "print(\"Starting calculation of maximum and minimum GHI values within each city area...\")\n",
    "stats = zonal_stats(\n",
    "    vectors=gdf_cities,\n",
    "    raster=tiff_file,\n",
    "    stats=['min', 'max'],\n",
    "    nodata=-9999,  # Set no data value to -9999\n",
    "    all_touched=True  # Include all pixels touched by polygons. By default False, considers only pixels whose center is within the polygon\n",
    ")\n",
    "\n",
    "# Add the results to the original GeoDataFrame\n",
    "gdf_cities['GHI_min'] = [stat['min'] for stat in stats]\n",
    "gdf_cities['GHI_max'] = [stat['max'] for stat in stats]  # Note: The original code had PVOUT_max here, but it's corrected to GHI_max as per the filename\n",
    "\n",
    "# Retain only the required columns (assuming 'NAME' is the field name for city names, replace if different)\n",
    "gdf_output = gdf_cities[['STATEFP', 'COUNTYFP', 'COUNTYNS', 'NAME', 'GHI_min', 'GHI_max']]\n",
    "\n",
    "# Save to CSV file, specifying utf-8 encoding to ensure non-ASCII characters are stored correctly\n",
    "gdf_output.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Processed data has been saved as: '{output_csv_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19892a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths (please adjust according to your actual file locations)\n",
    "charging_stations_file = 'filtered_USA_charging_stations.csv'\n",
    "pvout_stats_file = 'US_cities_GHI_min_max1.csv'\n",
    "output_csv_file = 'USAmatched_charging_stations_within_city_boundariesGHI_min_max.csv'\n",
    "\n",
    "# Read the charging stations data\n",
    "df_charging_stations = pd.read_csv(charging_stations_file)\n",
    "\n",
    "# Read the PVOUT statistics data\n",
    "df_pvout_stats = pd.read_csv(pvout_stats_file)\n",
    "\n",
    "# Ensure the 'COUNTYNS' column in both DataFrames has the same data type to avoid merge issues due to type mismatch\n",
    "df_charging_stations['COUNTYNS'] = df_charging_stations['COUNTYNS'].astype(str)\n",
    "df_pvout_stats['COUNTYNS'] = df_pvout_stats['COUNTYNS'].astype(str)\n",
    "\n",
    "# Merge the two DataFrames on the 'COUNTYNS' column\n",
    "df_merged = pd.merge(df_charging_stations, df_pvout_stats[['COUNTYNS', 'GHI_min', 'GHI_max']], on='COUNTYNS', how='left')\n",
    "\n",
    "# Save the merged result to a new CSV file\n",
    "df_merged.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Processed data has been saved as: '{output_csv_file}'\")\n",
    "\n",
    "# Print the first few rows to inspect the result\n",
    "print(\"First few rows of the merged data:\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0556c",
   "metadata": {},
   "source": [
    "# Maximum and minimum wind energy loading in USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = 'USAmatched_charging_stations_within_city_boundariesGHI_min_max.csv'\n",
    "shapefile_ghi_path = 'USA_city_wind\\\\USA_city_wind.shp'  # from External data\n",
    "output_csv_path = 'USAmatched_charging_stations_within_city_boundaries_GHIwind_maxmin.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df_charging_stations = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure the COUNTYNS field is a fixed-length string (e.g., 8 digits)\n",
    "df_charging_stations['COUNTYNS'] = df_charging_stations['COUNTYNS'].astype(str).str.zfill(8)\n",
    "\n",
    "# Read the Shapefile and convert CRS if necessary\n",
    "gdf_city_ghi = gpd.read_file(shapefile_ghi_path, engine='pyogrio')\n",
    "gdf_city_ghi.to_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Ensure the COUNTYNS field in the Shapefile is also a fixed-length string\n",
    "gdf_city_ghi['COUNTYNS'] = gdf_city_ghi['COUNTYNS'].astype(str).str.zfill(8)\n",
    "\n",
    "# Clean the COUNTYNS field by stripping any extra whitespace\n",
    "df_charging_stations['COUNTYNS'] = df_charging_stations['COUNTYNS'].str.strip()\n",
    "gdf_city_ghi['COUNTYNS'] = gdf_city_ghi['COUNTYNS'].str.strip()\n",
    "\n",
    "# Rename columns in the Shapefile for clarity before merging\n",
    "gdf_city_ghi.rename(columns={'lower': 'city_min_wind', 'upper': 'city_max_wind'}, inplace=True)\n",
    "\n",
    "# Check for missing values in the Shapefile\n",
    "missing_values = gdf_city_ghi[['COUNTYNS', 'city_min_wind', 'city_max_wind']].isna().sum()\n",
    "print(f\"Number of missing values in Shapefile:\\n{missing_values}\")\n",
    "\n",
    "# Inspect COUNTYNS values in both datasets after cleaning\n",
    "print(\"\\nCleaned COUNTYNS values in the CSV file:\")\n",
    "print(df_charging_stations['COUNTYNS'].unique())\n",
    "\n",
    "print(\"\\nCleaned COUNTYNS values in the Shapefile:\")\n",
    "print(gdf_city_ghi['COUNTYNS'].unique())\n",
    "\n",
    "# Merge the two datasets on the COUNTYNS field\n",
    "df_final = df_charging_stations.merge(\n",
    "    gdf_city_ghi[['COUNTYNS', 'city_min_wind', 'city_max_wind']],\n",
    "    on='COUNTYNS',\n",
    "    how='left'  # Use left join to ensure all original charging stations are retained\n",
    ")\n",
    "\n",
    "# Check for unmatched records\n",
    "unmatched_final = df_final[df_final['city_min_wind'].isna()]\n",
    "if not unmatched_final.empty:\n",
    "    print(f\"\\nThe following charging stations were not matched with city data:\\n{unmatched_final[['Station Name', 'City', 'STATEFP', 'COUNTYFP', 'COUNTYNS']]}\")\n",
    "else:\n",
    "    print(\"All charging stations were successfully matched with city data.\")\n",
    "\n",
    "# Verify that the number of rows in the final DataFrame matches the original\n",
    "if len(df_final) != len(df_charging_stations):\n",
    "    print(\"Warning: The number of charging stations in the final output does not match the original data.\")\n",
    "else:\n",
    "    print(\"Confirmed: The number of charging stations in the final output matches the original data.\")\n",
    "\n",
    "# Save the updated CSV file\n",
    "df_final.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"File has been successfully saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c213f",
   "metadata": {},
   "source": [
    "# Data quality inspection in USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86545c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "input_csv_path = 'USAmatched_charging_stations_within_city_boundaries_GHIwind_maxmin.csv'\n",
    "output_csv_path_updated = 'USA_charging_stations_with_updated_min_max_values.csv'\n",
    "\n",
    "# Read the input CSV file\n",
    "df_charging_stations = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Group by city (COUNTYNS) and calculate actual min and max values for GHI and wind\n",
    "grouped = df_charging_stations.groupby('COUNTYNS').agg(\n",
    "    actual_min_GHI=('GHI_value', 'min'),\n",
    "    actual_max_GHI=('GHI_value', 'max'),\n",
    "    actual_min_wind=('city_min_wind', 'min'),\n",
    "    actual_max_wind=('city_max_wind', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Create a copy of the original DataFrame to store updated values\n",
    "df_updated = df_charging_stations.copy()\n",
    "\n",
    "# Update min and max values in the original DataFrame based on actual station-level data\n",
    "for index, row in grouped.iterrows():\n",
    "    city_mask = df_charging_stations['COUNTYNS'] == row['COUNTYNS']\n",
    "    \n",
    "    # Update solar (GHI) min and max values if actual values are more extreme\n",
    "    if df_charging_stations.loc[city_mask, 'GHI_min'].iloc[0] > row['actual_min_GHI']:\n",
    "        df_updated.loc[city_mask, 'GHI_min'] = row['actual_min_GHI']\n",
    "    if df_charging_stations.loc[city_mask, 'GHI_max'].iloc[0] < row['actual_max_GHI']:\n",
    "        df_updated.loc[city_mask, 'GHI_max'] = row['actual_max_GHI']\n",
    "\n",
    "    # Update wind min and max values if actual values are more extreme\n",
    "    if df_charging_stations.loc[city_mask, 'city_min_wind'].iloc[0] > row['actual_min_wind']:\n",
    "        df_updated.loc[city_mask, 'city_min_wind'] = row['actual_min_wind']\n",
    "    if df_charging_stations.loc[city_mask, 'city_max_wind'].iloc[0] < row['actual_max_wind']:\n",
    "        df_updated.loc[city_mask, 'city_max_wind'] = row['actual_max_wind']\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_updated.to_csv(output_csv_path_updated, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Updated min/max values have been successfully saved to {output_csv_path_updated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07d783",
   "metadata": {},
   "source": [
    "# Calculation of matching level indicators in USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "updated_csv_path = 'USA_charging_stations_with_updated_min_max_values.csv'\n",
    "\n",
    "# Read the updated CSV file\n",
    "df_charging_stations = pd.read_csv(updated_csv_path)\n",
    "\n",
    "# Define wind power density threshold\n",
    "wind_power_density_threshold = 16.5375\n",
    "\n",
    "# Calculate PV matching level (Metric 1) for each charging station\n",
    "df_charging_stations['PV_matching_level'] = (\n",
    "    (df_charging_stations['GHI_value'] - df_charging_stations['GHI_min']) / \n",
    "    (df_charging_stations['GHI_max'] - df_charging_stations['GHI_min'])\n",
    ")\n",
    "\n",
    "# Identify charging stations with wind power density above or equal to the threshold\n",
    "df_charging_stations['is_wind_eligible'] = df_charging_stations['Power_Density'] >= wind_power_density_threshold\n",
    "\n",
    "# Group by city and filter out stations in cities with fewer than 5 eligible stations\n",
    "filtered_cities = []\n",
    "for city, group in df_charging_stations.groupby('COUNTYNS'):\n",
    "    eligible_group = group[group['is_wind_eligible']]\n",
    "    \n",
    "    if len(eligible_group) < 5:\n",
    "        # Skip cities with fewer than 5 wind-eligible charging stations\n",
    "        continue\n",
    "    \n",
    "    filtered_cities.append(eligible_group)\n",
    "\n",
    "# Combine data from all qualified cities\n",
    "df_filtered_charging_stations = pd.concat(filtered_cities, ignore_index=True)\n",
    "\n",
    "# Calculate wind matching level (Metric 1) for each eligible charging station\n",
    "df_filtered_charging_stations['wind_matching_level'] = (\n",
    "    (df_filtered_charging_stations['Power_Density'] - df_filtered_charging_stations['city_min_wind']) / \n",
    "    (df_filtered_charging_stations['city_max_wind'] - df_filtered_charging_stations['city_min_wind'])\n",
    ")\n",
    "\n",
    "# Calculate average PV matching level per city (Metric 2), station count, and average PV value\n",
    "city_avg_PV_matching = df_charging_stations.groupby('COUNTYNS').agg(\n",
    "    avg_PV_matching_level=('PV_matching_level', 'mean'),\n",
    "    charging_station_count=('PV_matching_level', 'size'),\n",
    "    avg_PV_value=('GHI_value', 'mean')  # New: compute average PV (GHI) value\n",
    ").reset_index()\n",
    "\n",
    "# Filter out cities with fewer than 5 total charging stations\n",
    "city_avg_PV_matching = city_avg_PV_matching[city_avg_PV_matching['charging_station_count'] >= 5]\n",
    "\n",
    "# Calculate average wind matching level per city (Metric 2), station count, and average Power Density\n",
    "city_avg_wind_matching = df_filtered_charging_stations.groupby('COUNTYNS').agg(\n",
    "    avg_wind_matching_level=('wind_matching_level', 'mean'),\n",
    "    charging_station_count=('wind_matching_level', 'size'),\n",
    "    avg_power_density=('Power_Density', 'mean')  # New: compute average Power Density\n",
    ").reset_index()\n",
    "\n",
    "# Save individual PV matching levels (Metric 1) to CSV\n",
    "output_csv_path_1 = 'USA_charging_stations_PV_matching_level_city_based11.csv'\n",
    "df_charging_stations[['STATEFP', 'COUNTYFP', 'NAME', 'COUNTYNS', 'GHI_value', 'PV_matching_level']].to_csv(output_csv_path_1, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Save city-level average PV matching level (Metric 2), station count, and average PV value to CSV\n",
    "output_csv_path_2 = 'USA_city_avg_PV_matching_level_with_count_city_based_filtered22.csv'\n",
    "city_avg_PV_matching.to_csv(output_csv_path_2, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Save individual wind matching levels (Metric 1) for eligible stations to CSV\n",
    "output_csv_path_3 = 'USA_charging_stations_wind_matching_level_city_based11.csv'\n",
    "df_filtered_charging_stations[['STATEFP', 'COUNTYFP', 'NAME', 'COUNTYNS', 'Power_Density', 'wind_matching_level']].to_csv(output_csv_path_3, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Save city-level average wind matching level (Metric 2), station count, and average Power Density to CSV\n",
    "output_csv_path_4 = 'USA_city_avg_wind_matching_level_with_count_city_based_filtered22.csv'\n",
    "city_avg_wind_matching.to_csv(output_csv_path_4, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"File successfully saved to {output_csv_path_1}\")\n",
    "print(f\"File successfully saved to {output_csv_path_2}\")\n",
    "print(f\"File successfully saved to {output_csv_path_3}\")\n",
    "print(f\"File successfully saved to {output_csv_path_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a71b6ca",
   "metadata": {},
   "source": [
    "# Based on the above description of programming in USA, data from China and Europe can be obtained. Then, integrating the indicators of China, USA and Europe vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "china_csv_path = 'China_city_avg_PV_matching_level_with_count_city_based_filtered222.csv'##Obtain the code similar to the one in USA mentioned above\n",
    "eu_csv_path = 'EU_city_avg_PV_matching_level_with_count_city_based_filtered22.csv'##Obtain the code similar to the one in USA mentioned above\n",
    "usa_csv_path = 'USA_city_avg_PV_matching_level_with_count_city_based_filtered22.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "df_china = pd.read_csv(china_csv_path)\n",
    "df_eu = pd.read_csv(eu_csv_path)\n",
    "df_usa = pd.read_csv(usa_csv_path)\n",
    "\n",
    "# Add an 'Area' column to identify the region\n",
    "df_china['Area'] = 'China'\n",
    "df_eu['Area'] = 'EU'\n",
    "df_usa['Area'] = 'USA'\n",
    "\n",
    "# Standardize column names to ensure proper merging\n",
    "df_china.rename(columns={'city': 'City'}, inplace=True)\n",
    "df_eu.rename(columns={'NUTS_NAME_right': 'City'}, inplace=True)\n",
    "df_usa.rename(columns={'COUNTYNS': 'City'}, inplace=True)\n",
    "\n",
    "# Ensure all DataFrames have the same column order\n",
    "columns_order = ['City', 'avg_PV_matching_level', 'charging_station_count', 'avg_PV_value', 'Area']\n",
    "df_china = df_china[columns_order]\n",
    "df_eu = df_eu[columns_order]\n",
    "df_usa = df_usa[columns_order]\n",
    "\n",
    "# Concatenate the three DataFrames\n",
    "df_combined = pd.concat([df_china, df_eu, df_usa], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "output_csv_path = 'combined_city_avg_PV_matching_level_with_area22.csv'\n",
    "df_combined.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Combined file has been successfully saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fa2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "china_csv_path = 'China_city_avg_wind_matching_level_with_count_city_based_filtered222.csv'\n",
    "eu_csv_path = 'EU_city_avg_wind_matching_level_with_count_city_based_filtered22.csv'\n",
    "usa_csv_path = 'USA_city_avg_wind_matching_level_with_count_city_based_filtered22.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "df_china = pd.read_csv(china_csv_path)\n",
    "df_eu = pd.read_csv(eu_csv_path)\n",
    "df_usa = pd.read_csv(usa_csv_path)\n",
    "\n",
    "# Add an 'Area' column to identify the region\n",
    "df_china['Area'] = 'China'\n",
    "df_eu['Area'] = 'EU'\n",
    "df_usa['Area'] = 'USA'\n",
    "\n",
    "# Standardize column names to ensure proper merging\n",
    "df_china.rename(columns={'city': 'City'}, inplace=True)\n",
    "df_eu.rename(columns={'NUTS_NAME_right': 'City'}, inplace=True)\n",
    "df_usa.rename(columns={'COUNTYNS': 'City'}, inplace=True)\n",
    "\n",
    "# Ensure all DataFrames have the same column order\n",
    "columns_order = ['City', 'avg_wind_matching_level', 'charging_station_count', 'avg_power_density', 'Area']\n",
    "df_china = df_china[columns_order]\n",
    "df_eu = df_eu[columns_order]\n",
    "df_usa = df_usa[columns_order]\n",
    "\n",
    "# Concatenate the three DataFrames\n",
    "df_combined = pd.concat([df_china, df_eu, df_usa], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "output_csv_path = 'combined_city_avg_wind_matching_level_with_area22.csv'\n",
    "df_combined.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Combined file has been successfully saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacf128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "ghi_csv_path = 'combined_city_avg_PV_matching_level_with_area22.csv'\n",
    "wind_csv_path = 'combined_city_avg_wind_matching_level_with_area22.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "df_ghi = pd.read_csv(ghi_csv_path)\n",
    "df_wind = pd.read_csv(wind_csv_path)\n",
    "\n",
    "# Merge wind data into the solar (GHI) dataset\n",
    "# Use a left join to ensure all cities from the solar dataset are retained\n",
    "df_merged = pd.merge(\n",
    "    df_ghi,  # Solar (GHI) data as the primary table\n",
    "    df_wind[['City', 'avg_wind_matching_level', 'avg_power_density']],  # Select only relevant wind columns\n",
    "    on='City',  # Merge on city name\n",
    "    how='left'  # Left join: keep all cities from the solar dataset\n",
    ")\n",
    "\n",
    "# Fill missing wind values with NaN where no matching city data exists\n",
    "df_merged['avg_wind_matching_level'] = df_merged['avg_wind_matching_level'].fillna(pd.NA)\n",
    "df_merged['avg_power_density'] = df_merged['avg_power_density'].fillna(pd.NA)\n",
    "\n",
    "# Display the first few rows of the merged dataset\n",
    "print(df_merged.head())\n",
    "\n",
    "# Save the merged dataset to a new CSV file (optional)\n",
    "df_merged.to_csv('combined_city_PV_wind_matching_level_with_area22.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad6d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "157690cc",
   "metadata": {},
   "source": [
    "# Calculate the wind and solar power generation of EVCS in USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64512bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data_file_path = 'USA_charging_stations_with_updated_min_max_values.csv'\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Define constants\n",
    "swept_area = 2.7  # m^2, swept area of the wind turbine\n",
    "efficiency = 0.35  # Wind turbine efficiency\n",
    "hours_per_year = 3000  # Number of hours per year (estimated operational hours)\n",
    "days_per_year = 365  # Number of days per year\n",
    "conversion_efficiency = 0.18  # Photovoltaic (PV) panel conversion efficiency (e.g., 18%)\n",
    "wind_power_density_threshold = 16.5375  # Wind power density threshold in W/m²\n",
    "\n",
    "# Function to convert wind power density to annual energy generation\n",
    "def wind_energy_potential(power_density):\n",
    "    energy_per_year = power_density * swept_area * efficiency * hours_per_year / 1000  # Convert to kWh\n",
    "    return energy_per_year\n",
    "\n",
    "# Function to convert GHI (Global Horizontal Irradiance) to annual PV energy generation\n",
    "def pv_energy_potential(ghi_value):\n",
    "    energy_per_year = ghi_value * days_per_year * conversion_efficiency  # kWh/m²/year\n",
    "    return energy_per_year\n",
    "\n",
    "# Group data by city (COUNTYNS)\n",
    "grouped = df.groupby('COUNTYNS')\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Counter for the number of valid cities (for wind generation only)\n",
    "valid_city_count_wind = 0\n",
    "\n",
    "for county, group in grouped:\n",
    "    # Filter charging stations with wind power density above the threshold\n",
    "    can_wind_generate_group = group[group['Power_Density'] >= wind_power_density_threshold]\n",
    "    \n",
    "    if len(can_wind_generate_group) >= 5:\n",
    "        # If at least 5 stations meet the wind condition, calculate average wind energy\n",
    "        total_wind_energy = can_wind_generate_group['Power_Density'].apply(wind_energy_potential).sum()\n",
    "        avg_wind_energy = total_wind_energy / len(can_wind_generate_group)\n",
    "        valid_city_count_wind += 1\n",
    "    else:\n",
    "        # If fewer than 5 stations meet the condition, set average wind energy to NaN\n",
    "        avg_wind_energy = float('nan')\n",
    "    \n",
    "    # Calculate average PV energy for all stations in the city\n",
    "    total_pv_energy = group['GHI_value'].apply(pv_energy_potential).sum()\n",
    "    avg_pv_energy = total_pv_energy / len(group)\n",
    "    \n",
    "    results.append({\n",
    "        'COUNTYNS': county,\n",
    "        'Avg_Wind_Energy': avg_wind_energy,\n",
    "        'Avg_PV_Energy': avg_pv_energy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "# Add the count of valid cities (for wind) as a separate column (broadcasted)\n",
    "result_df['Valid_City_Count_Wind'] = valid_city_count_wind\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_file_path = 'USAcity_renewable_energy_potential_filtered.csv'\n",
    "result_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1fd9a",
   "metadata": {},
   "source": [
    "# Based on the above description of programming in USA, data from China and Europe can be obtained. Merge the wind and solar energy of the three regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "eu_file_path = 'EUcity_renewable_energy_potential_filtered.csv'  # Code derived similarly to the USA version\n",
    "china_file_path = 'Chinacity_renewable_energy_potential_filtered.csv'  # Code derived similarly to the USA version\n",
    "usa_file_path = 'USAcity_renewable_energy_potential_filtered.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "eu_df = pd.read_csv(eu_file_path)\n",
    "china_df = pd.read_csv(china_file_path)\n",
    "\n",
    "# Read the USA data and rename the column to standardize\n",
    "usa_df = pd.read_csv(usa_file_path)\n",
    "usa_df.rename(columns={'COUNTYNS': 'City'}, inplace=True)\n",
    "\n",
    "# Concatenate the three DataFrames\n",
    "combined_df = pd.concat([eu_df, china_df, usa_df], ignore_index=True)\n",
    "\n",
    "# Save the combined results to a new CSV file\n",
    "output_file_path = 'combined_renewable_energy_potential.csv'\n",
    "combined_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Combined results saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e7fe4b",
   "metadata": {},
   "source": [
    "# The energy and matching levels of wind and solar in the three regions are combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "renewable_energy_file_path = 'combined_renewable_energy_potential.csv'\n",
    "coverage_level_file_path = 'combined_city_PV_wind_matching_level_with_area22.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "renewable_energy_df = pd.read_csv(renewable_energy_file_path)\n",
    "coverage_level_df = pd.read_csv(coverage_level_file_path)\n",
    "\n",
    "# Merge the two DataFrames horizontally based on the 'City' column\n",
    "merged_df = pd.merge(renewable_energy_df, coverage_level_df, on='City', how='outer')\n",
    "\n",
    "# Save the merged result to a new CSV file\n",
    "output_file_path = 'combineddata_windsolarenergy_and_matchinglevel.csv'\n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Merged results saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
